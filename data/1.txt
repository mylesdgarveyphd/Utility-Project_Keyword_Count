InfoQ Dev Summit (June 24-25): Get practical advice to make development decisions easier and less risky.
Register Now




AI, ML & DATA ENGINEERING
Microsoft Announces Small Language Model Phi-2
DEC 19, 2023 2 MIN READ

by


Anthony Alford
Director, Development at Genesys Cloud Services
Microsoft Research announced Phi-2, a 2.7 billion-parameter Transformer-based language model. Phi-2 is trained on 1.4T tokens of synthetic data generated by GPT-3.5 and outperforms larger models on a variety of benchmarks.

Phi-2 is the latest iteration of Microsoft's Phi suite of models, which are trained on a mixture of web-crawled and synthetic "textbook-quality" datasets. The previous Phi models contain only 1.3B parameters, but showed excellent performance on coding and reasoning tasks. Phi-2 is twice as large as previous ones and was trained for two weeks on a cluster of 96 A100 GPUs. It has performance comparable to models which are up to 25x larger, outperforming the 70B parameter Llama-2 model on reasoning, language understanding, and coding benchmarks. According to Microsoft:

With its compact size, Phi-2 is an ideal playground for researchers, including for exploration around mechanistic interpretability, safety improvements, or fine-tuning experimentation on a variety of tasks. We have made Phi-2 available in the Azure AI Studio model catalog to foster research and development on language models.

InfoQ recently covered several efforts to replicate the abilities of large language models (LLMs) in smaller models. Many of these use LLMs such as ChatGPT to generate synthetic training datasets for the smaller model. Google's Distilling Step-by-Step method prompts a teacher LLM to automatically generate a small fine-tuning dataset that contains both an input with an output label, as well as a "rationale" for why the output label was chosen. Microsoft Research's Orca 2 uses a synthetic training dataset and a new technique called Prompt Erasure to achieve performance equal to or better than models that contain 10x the number of parameters.

The key innovation with the Phi series of models is a synthetic dataset of "textbook-like" data. Although the researchers have not released the dataset or even very many details of its generation, previous tech reports on the Phi models include high-level descriptions. One goal for the datasets was to generate "diverse and non-repetitive" examples that cover a range of "concepts, skills, and scenarios" that vary in "level of difficulty, complexity, and style." For Phi-1.5, the team selected 20k different topics for generated examples of language understanding problems. 

Sebastien Bubeck, lead ML foundations team at Microsoft Research, posted on X about some additional work fine-tuning Phi-2:

phi-2 is really a good base for further fine-tuning: we [fine-tune] on 1M math exercises (similar to phi-1 w. CodeExercises) & test on recent French nation-wide math exam (published after phi-2 finished training). The results are encouraging! Go try your own data...

Mark Tenenholtz, the head of AI at Predelo, also posted about Phi-2, that "knowledge distillation really does work." In a Hacker News discussion about Phi-2, one user noted that the compute cost of training the model was probably around 30k USD, or "cheaper than a car." Another pointed out:

Note the model is trained on data generated by GPT-4. It's probably orders of magnitude more expensive to generate the data at current API prices. The whole point of these papers is that training data quality is key. I would much prefer for these companies to release the training data than the weights.

The Phi-2 model weights are available on HuggingFace.

About the Author

Anthony Alford
Anthony is a Director, Development at Genesys where he is working on several AI and ML projects related to customer experience. He has over 20 years experience in designing and building scalable software. Anthony holds a Ph.D. degree in Electrical Engineering with specialization in Intelligent Robotics Software and has worked on various problems in the areas of human-AI interaction and predictive analytics for SaaS business optimization.

Show more
This content is in the AI, ML & Data Engineering topic
Related Topics: 

 

 

 

 

 

 

POPULAR IN AI, ML & DATA ENGINEERING
How Netflix Ensures Highly-Reliable Online Stateful Systems
OpenAI Launches AI Text-to-Video Generator Sora
Java News Roundup: JDK 22 RC1, JBoss EAP 8.0, GlassFish 8.0-M2, LangChain4j 0.27
Microsoft Shares Lessons Learned on Building AI Copilots
Java News Roundup: New JEP Candidates, Milestone Releases for Spring Projects and Micrometer
RELATED SPONSORED CONTENT
Using Deduplication for Eventually Consistent Transactions
[eBook] Kubernetes Best Practices, 2nd Edition
Architecting Distributed Transactional Applications - Download Now (By O'Reilly)
Why choose a purpose-built time series database?
Optimizing Multi-Region Database Deployment: Achieving Consistency, Performance, and Availability in Modern Applications
RELATED SPONSOR

Feb 29, 2024, 1 PM EST
Tame the complexities of generative AI and LLMs
Presented by: Srikanth Venkata Seshu - Head of Product, HPE Ezmeral Unified Analytics Software and Joey Zwicker - VP of HPE AI portfolio and sales strategy

SPONSORED BY HPE
Save Your Seat



Please enter a subject
Message
Allowed html: a,b,br,blockquote,i,li,pre,u,ul,p

Community comments
DEVELOPMENT
Loco Is a New Framework for Rust Inspired by Rails
Microsoft Copilot Studio Enables AI-Driven Conversational Interfaces for Business Applications
InfoQ & QCon Events: Level up on Generative AI, Security, Platform Engineering, and More Upcoming
ARCHITECTURE & DESIGN
Grab Improves Kafka on Kubernetes Fault Tolerance with Strimzi, AWS AddOns and EBS
Orchestrating Resilience: Building Modern Asynchronous Systems
AWS Lambda Under the Hood
CULTURE & METHODS
4 Steps to Achieving Operational Flow and Improving Quality in Tech Teams
Agile Rehab: Engineering for Improved Delivery
Becoming a Staff Plus Engineer with Joy Ebertz
AI, ML & DATA ENGINEERING
Google Renames Bard to Gemini
OpenAI is Adding Memory Capabilities to ChatGPT to Improve Conversations
Google Announces Multi-Modal Gemini 1.5 with Million Token Context Length
DEVOPS
AI and FinOps Predicted to Lead Observability Innovation in 2024
Zurich Insurance Group's Journey with Scalable Account Vending and AWS Account Factory for Terraform
Mastering the Art of Platform Engineering: Perspectives from Industry Practitioners
The InfoQ Newsletter
A round-up of last week’s content on InfoQ sent out every Tuesday. Join a community of over 250,000 senior developers. View an example

Get a quick overview of content published on a variety of innovator and early adopter technologies
Learn what you don’t know that you don’t know
Stay up to date with the latest information from the topics you are interested in
Enter your e-mail address
We protect your privacy.


June 24 - 25, 2024
Actionable insights to clarify today's critical dev priorities.
InfoQ Dev Summit Boston, is a two-day conference hosted by InfoQ, focusing on the most critical technical decisions senior software developers face today.
Deep-dive into 20+ technical talks and get transformative learnings from senior software developers navigating Generative AI, security, modern web applications, and more.

Home
Create Account
QCon Conferences
Events
Write For InfoQ
InfoQ Editors
About InfoQ
About C4Media
Media Kit
InfoQ Developer Marketing Blog
Diversity
Events
QCon London
APRIL 8-10, 2024
InfoQ Dev Summit Boston
JUNE 24-25, 2024
QCon San Francisco
NOVEMBER 18-22, 2024
Follow us on
Youtube
223K Followers
Linkedin
21K Followers
RSS
19K Readers
X
53.4k Followers
Facebook
21K Likes
Alexa
New
Stay in the know
The InfoQ Podcast
The InfoQ Podcast
Engineering Culture Podcast
Engineering Culture Podcast
The Software Architects' Newsletter
The Software Architects' Newsletter
General Feedback
feedback@infoq.com
Advertising
sales@infoq.com
Editorial
editors@infoq.com
Marketing
marketing@infoq.com
InfoQ.com and all content copyright © 2006-2023 C4Media Inc.
Privacy Notice, Terms And Conditions, Cookie Policy

