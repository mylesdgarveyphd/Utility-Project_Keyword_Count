word,color_hex
2,#8C0000
phi,#8C0000
model,#F20000
phi 2,#000000
models,#260000
data,#190000
language,#260000
ai,#190000
training,#3F0000
days,#000000
ago,#000000
microsoft,#190000
days ago,#000000
parent,#000000
root,#000000
ago root,#000000
ago root parent,#000000
days ago root,#000000
days ago root parent,#000000
root parent,#000000
of the,#000000
the model,#000000
fine,#190000
in the,#000000
parent next,#000000
language models,#000000
ago root parent next,#000000
code,#260000
days ago root parent next,#000000
parameters,#0C0000
root parent next,#000000
language model,#000000
llm,#0C0000
performance,#190000
70,#000000
prev,#000000
billion,#0C0000
70 days,#000000
70 days ago,#000000
prev next,#000000
research,#000000
4,#0C0000
llama,#0C0000
quality,#0C0000
tuning,#0C0000
itâ€™s,#000000
small language,#000000
gpt,#000000
fine tuning,#000000
kind of,#000000
1,#260000
size,#0C0000
you can,#000000
2 is,#000000
source,#0C0000
synthetic,#0C0000
is a,#000000
phi 2 is,#000000
you know,#000000
on the,#000000
tasks,#000000
llms,#000000
reasoning,#0C0000
true,#190000
read,#000000
parent prev,#000000
to the,#000000
2024,#000000
trained,#0C0000
human,#000000
parent prev next,#000000
prompt,#260000
1.5,#0C0000
learning,#0C0000
gpu,#0C0000
training data,#000000
is the,#000000
2023,#000000
45,#000000
45 days,#000000
45 days ago,#000000
benchmarks,#0C0000
phi 1.5,#000000
as well,#000000
dataset,#3F0000
https,#000000
larger,#0C0000
text,#0C0000
the training,#000000
tokens,#0C0000
70 days ago root,#000000
70 days ago root parent,#000000
bit,#000000
i think,#000000
knowledge,#0C0000
run,#0C0000
this is,#000000
to be,#000000
understanding,#0C0000
coding,#0C0000
