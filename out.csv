word
2
phi
model
phi 2
models
data
language
ai
training
days
ago
microsoft
days ago
parent
root
ago root
ago root parent
days ago root
days ago root parent
root parent
of the
the model
fine
in the
parent next
language models
ago root parent next
code
days ago root parent next
parameters
root parent next
language model
llm
performance
70
prev
billion
70 days
70 days ago
prev next
research
4
llama
quality
tuning
itâ€™s
small language
gpt
fine tuning
kind of
1
size
you can
2 is
source
synthetic
is a
phi 2 is
you know
on the
tasks
llms
reasoning
true
read
parent prev
to the
2024
trained
human
parent prev next
prompt
1.5
learning
gpu
training data
is the
2023
45
45 days
45 days ago
benchmarks
phi 1.5
as well
dataset
https
larger
text
the training
tokens
70 days ago root
70 days ago root parent
bit
i think
knowledge
run
this is
to be
understanding
coding
